# Manufacturing Lakebase Demo

A demonstration of Databricks Lakebase capabilities for shop floor routing management. This app shows how to use Lakebase to expose analytical data for low-latency queries and support transactional workloads.

## üè≠ What This Demo Shows

- **Sync data from Unity Catalog to Lakebase**
- **Write data directly to Lakebase**
- **Integrate Lakebase with Databricks Apps**

## üöÄ Quick Start

1. **Open this repo in your Databricks workspace**
   - Create a `Git folder` or upload the folders.

2. **Generate Sample Data**
   - Run `dummy_data_gen/data_gen.ipynb` to generate sample manufacturing data.
   - This produces the following table:

     **`recommended_routes`**  
     _Table Type: Unity Catalog Delta Table_

     > This table contains mock data representing recommended routing assignments for each part on the factory floor, including the specific machine each part should be processed on. Assignments may be generated by a machine learning model, upstream application, or business logic, representing analytical data stored in the Databricks Lakehouse. We sync this table to Lakebase for low-latency access in downstream applications (e.g., Factory Routing App deployed on Databricks Apps).

     - `part_id`: Part identifier
     - `priority`: Manufacturing priority
     - `quantity_pending`: Units to be produced
     - `due_date`: Production deadline
     - `recommended_machine_id`: AI-suggested machine
     - `route_confidence`: Confidence score

3. **Setup Lakebase**
   - Run `lakebase_setup/lakebase_setup.ipynb` to create a Lakebase instance and these tables:

     **`recommended_routes_synced_table`**  
     _Table Type: Lakebase Synced Table (read-only in Lakebase)_

     > Synchronizes data from the Unity Catalog `recommended_routes` table into a Lakebase table. Sync is managed by Databricks and can be configured for data freshness.

     - `part_id`
     - `priority`
     - `quantity_pending`
     - `due_date`
     - `recommended_machine_id`
     - `route_confidence`

     **`assignment_overrides`**  
     _Native Lakebase Table (read/write)_

     > Captures manual assignment overrides by shop floor managers in response to real-world shop floor conditions (e.g., machine downtime, special part handling). Demonstrates how applications can write operational workloads directly to Lakebase.

     - `part_id`
     - `assigned_machine_id`
     - `assigned_by`
     - `assigned_at`
     - `notes`

4. **Create Factory Routing App**
   - In the Databricks Workspace UI:
     - Go to Compute > Apps > Create new app.
     - Configure the App with a Database resource, selecting the appropriate Lakebase instance/database.
     - Go to the app‚Äôs Authorization tab and note the Service Principal Client ID.

5. **Grant Permissions in Lakebase**
   - In the Workspace UI:
     - Navigate to your Lakebase instance > Click `New Query`.
     - Ensure your database/schema is selected.
     - Run the following grants:
       ```
       GRANT USAGE ON SCHEMA your_lakebase_schema TO "your-service-principal-client-id";
       GRANT SELECT ON your_lakebase_schema.recommended_routes_synced_table TO "your-service-principal-client-id";
       GRANT INSERT, SELECT ON your_lakebase_schema.assignment_overrides TO "your-service-principal-client-id";
       ```
     - _(Replace `your_lakebase_schema` and `your-service-principal-client-id`.)_

6. **Deploy the App**
   - Deploy your app, setting the deployment/source code path to the `shop_floor_app` folder.

## üìÅ Project Structure

