# Manufacturing Lakebase Demo

A demonstration of Databricks Lakebase capabilities for shop floor routing management. This app shows how to use Lakebase to expose analytical data for low latency queries and support transactional workloads.

## ğŸ­ What This Demo Shows

- **Sync data from Unity Catalog to Lakebase** 
- **Write data directly to Lakebase** 
- **Integrate Lakebase with Databricks Apps**

## ğŸš€ Quick Start

### 1. Setup Data Pipeline

1. Open this repo in your Databricks workspace by creating a `Git folder` or uploading the folders 
2. **Run `dummy_data_gen/data_gen.ipynb`** to generate sample manufacturing data. This will result in the following table:

**`recommended_routes`**
Description: This Unity Catalog Delta Table contains mock data representing recommended routing assignments for each part on the factory floor, including the specific machine each part should be processed on. The assignments could be generated by a machine learning model, upstream application, or other logic, and exemplify analytical data stored in the Databricks Lakehouse. To enable low-latency access, we will create a synced table in Lakebase making the data available to downstream applications (represented by our Factory Routing App deployed on Databricks Apps).
- `part_id`: Part identifier
- `priority`: Manufacturing priority
- `quantity_pending`: Units to be produced
- `due_date`: Production deadline
- `recommended_machine_id`: AI-suggested machine
- `route_confidence`: Confidence score

3. **Run `lakebase_setup/lakebase_setup.ipynb`** to create a lakebase instance and the following lakebase tables:
**`recommended_routes_synced_table`** 
Description: This synced table is a read-only table in Lakebase that synchronizes data from the Unity Catalog `recommended_routes` table. The synchronization is managed by Databricks, but the sync frequency can be configured based on data freshness requirements. 
- `part_id`: Part identifier
- `priority`: Manufacturing priority
- `quantity_pending`: Units to be produced
- `due_date`: Production deadline
- `recommended_machine_id`: AI-suggested machine
- `route_confidence`: Confidence score

**`assignment_overrides`**
Description: This native Lakebase table captures manual routing assignment overrides made by shop floor managers in response to real-world conditions, such as unexpected machine downtime or special handling requirements for certain parts. Each override entry records the part, the manually assigned machine, who made the override, the timestamp, and any relevant notes. This demonstrates how interactive applications can write operational decisions directly to Lakebase. 
- `part_id`: Part identifier
- `assigned_machine_id`: Manually chosen machine
- `assigned_by`: User who made the decision
- `assigned_at`: Timestamp of decision
- `notes`: Additional context

4. **Create the Factory Routing Databricks App using the Workspace UI**:
   - In the Databricks Workspace, navigate to the Compute tab > Apps > Create new app
   - Make sure to configure the App with a Database resource, selecting the appropriate Lakebase instance and database
   - Once the app is created, go to the authorization tab and make note of the Service Principal Client ID

5. **Grant permissions to the Databricks App Service Principal to Lakebase**
   - In the Databricks Workspace UI, navigate to your Lakebase instance and then click `New Query ` in the top right corner
   - Make sure your Postgres Database and Schema are selected
   - Run the following SQL Grants

`GRANT USAGE ON SCHEMA your_lakebase_schema TO "your-service-principal-client-id";`

`GRANT SELECT ON your_lakebase_schema.recommended_routes_synced_table TO "your-service-principal-client-id";`

`GRANT INSERT, SELECT ON your_lakebase_schema.assignment_overrides TO "your-service-principal-client-id";`

6. **Deploy the App**
- Deploy your app and set the deployment/source code path to the `shop_floor_app` folder

## ğŸ“ Project Structure

```
mfg_lakebase_demo/
â”œâ”€â”€ shop_floor_app/           # Streamlit web application
â”‚   â”œâ”€â”€ app.py               # Main application code
â”‚   â”œâ”€â”€ app.yaml             # Databricks App configuration
â”‚   â””â”€â”€ requirements.txt     # Python dependencies
â”œâ”€â”€ lakebase_setup/          # Data pipeline setup
â”‚   â””â”€â”€ lakebase_setup.ipynb # DLT pipeline and data creation
â”œâ”€â”€ dummy_data_gen/          # Sample data generation
â”‚   â””â”€â”€ data_gen.ipynb      # Manufacturing data creation
â”œâ”€â”€ .envrc.example          # Environment variables template
â””â”€â”€ README.md               # This file
```