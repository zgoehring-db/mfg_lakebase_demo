# Manufacturing Lakebase Demo

A demonstration of Databricks Lakebase capabilities for shop floor routing management. This app shows how to use Lakebase to expose analytical data for low-latency queries and support transactional workloads.

## ‚ö° What This Demo Shows

- **Sync data from Unity Catalog to Lakebase**
- **Write data directly to Lakebase**
- **Integrate Lakebase with Databricks Apps**

## üöÄ Quick Start

1. **Open this repo in your Databricks workspace**
   - Create a `Git folder` or upload the folders.

2. **Generate Sample Data**
   - Run `dummy_data_gen/data_gen.ipynb` to generate sample manufacturing data.
   - This produces the following table:

     **`recommended_routes`**  
     _Table Type: Unity Catalog Delta Table_

     > This table contains mock data representing recommended routing assignments for each part on the factory floor, including the specific machine each part should be processed on. Assignments may be generated by a machine learning model, upstream application, or business logic, representing analytical data stored in the Databricks Lakehouse. We sync this table to Lakebase for low-latency access. In this example, our Shop Floor Routing app displays all these routes so the shop floor manager has visibility into routing assignments.

     - `part_id`: Part identifier
     - `priority`: Manufacturing priority
     - `quantity_pending`: Units to be produced
     - `due_date`: Production deadline
     - `recommended_machine_id`: AI-suggested machine
     - `route_confidence`: Confidence score
    
     **`part_backlog`**  
     _Table Type: Unity Catalog Delta Table_

     > This table contains mock data representing all of the parts in the backlog, with detailed information about each part such as the material, finish, weight, etc. We also sync this table to Lakebase for low-latency access. In this example, our Shop Floor Routing app can issue a part lookup to fetch all of the part details for a given part. 

     - `part_id`: Unique part identifier
     - `priority`: Manufacturing priority level (high, medium, low)
     - `quantity_pending`: Number of units to be produced
     - `due_date`: Production deadline date
     - `material`: Raw material type (e.g., steel, aluminum, plastic)
     - `part_type`: Category of part (e.g., bracket, housing, connector)
     - `quality_level`: Required quality standard (e.g., A, B, C grade)
     - `surface_finish`: Surface treatment specification (e.g., anodized, painted, raw)
     - `tolerance`: Dimensional tolerance requirements (e.g., ¬±0.1mm)
     - `weight_kg`: Part weight in kilograms
     - `dimensions`: Physical dimensions (length x width x height)
     - `drawing_number`: Engineering drawing reference number
     - `revision`: Drawing revision level (e.g., Rev A, Rev B)
     - `estimated_hours`: Estimated manufacturing time in hours
    
3. **Setup Lakebase**
   - Run `lakebase_setup/lakebase_setup.ipynb` to create a Lakebase instance and these tables:

     **`recommended_routes_synced_table`**  
     _Table Type: Lakebase Synced Table (read-only in Lakebase)_

     > Synchronizes data from the Unity Catalog `recommended_routes` table into a Lakebase table. Sync is managed by Databricks and can be configured for data freshness.

     - `part_id`
     - `priority`
     - `quantity_pending`
     - `due_date`
     - `recommended_machine_id`
     - `route_confidence`
     
     **`part_backlog_synced_table`**  
     _Table Type: Lakebase Synced Table (read-only in Lakebase)_
      
     > Synchronizes data from the Unity Catalog `recommended_routes` table into a Lakebase table. Sync is managed by Databricks and can be configured for data freshness.
     
     - `part_id`
     - `priority`
     - `quantity_pending`
     - `due_date`
     - `material`
     - `part_type`
     - `quality_level`
     - `surface_finish`
     - `tolerance`
     - `weight_kg`
     - `dimensions`
     - `drawing_number`
     - `revision`
     - `estimated_hours`

     **`assignment_overrides`**  
     _Native Lakebase Table (read/write)_

     > Captures manual assignment overrides by shop floor managers in response to real-world shop floor conditions (e.g., machine downtime, special part handling). Demonstrates how applications can write operational workloads directly to Lakebase.

     - `part_id`
     - `assigned_machine_id`
     - `assigned_by`
     - `assigned_at`
     - `notes`

5. **Create Factory Routing App**
   - In the Databricks Workspace UI:
     - Go to Compute > Apps > Create new app.
     - Configure the App with a Database resource, selecting the appropriate Lakebase instance/database.
     - Go to the app‚Äôs Authorization tab and note the Service Principal Client ID.

6. **Grant Permissions in Lakebase**
   - In the Workspace UI:
     - Navigate to your Lakebase instance > Click `New Query`.
     - Ensure your database/schema is selected.
     - Run the following grants:
       ```
       GRANT USAGE ON SCHEMA your_lakebase_schema TO "your-service-principal-client-id";
       GRANT SELECT ON your_lakebase_schema.recommended_routes_synced_table TO "your-service-principal-client-id";
       GRANT INSERT, SELECT ON your_lakebase_schema.assignment_overrides TO "your-service-principal-client-id";
       ```
     - _(Replace `your_lakebase_schema` and `your-service-principal-client-id`.)_

7. **Deploy the App**
   - Deploy your app, setting the deployment/source code path to the `shop_floor_app` folder.

## üè≠ App Overview
- Tab 1: displays all the recommended part to machine routes from `recommended_routes_synced_table`
- Tab 2: allows the user to look up details for any part from `part_backlog_synced_table`
- Tab 3: allows the user to submit overrides directly to `assignment_overrides` 
